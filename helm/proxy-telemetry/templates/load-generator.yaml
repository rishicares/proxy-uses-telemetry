{{- if .Values.loadGenerator.enabled }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: load-generator-script
  namespace: {{ .Values.global.crawlersNamespace }}
  labels:
    app: load-generator
data:
  load-generator.py: |
    #!/usr/bin/env python3
    """
    Production-grade load generator for proxy telemetry testing.
    Simulates realistic crawler behavior with multiple vendors.
    """
    import asyncio
    import aiohttp
    import os
    import random
    import sys
    import logging
    from datetime import datetime
    from typing import List, Dict
    import json

    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)

    # Configuration
    PROXY_VENDOR = os.environ.get('PROXY_VENDOR', 'unknown')
    ENVOY_PROXY = f"http://localhost:{{ .Values.envoy.proxyPort }}"
    REQUESTS_PER_SECOND = int(os.environ.get('REQUESTS_PER_SECOND', '{{ .Values.loadGenerator.traffic.requestsPerSecond }}'))
    CONCURRENT_REQUESTS = int(os.environ.get('CONCURRENT_REQUESTS', '{{ .Values.loadGenerator.traffic.concurrentRequests }}'))
    HTTP_RATIO = float(os.environ.get('HTTP_RATIO', '{{ .Values.loadGenerator.traffic.httpRatio }}'))

    # Target destinations
    DESTINATIONS = {{ .Values.loadGenerator.destinations | toJson }}

    # Additional realistic targets
    ADDITIONAL_TARGETS = [
        "http://httpbin.org/get",
        "http://httpbin.org/headers",
        "http://httpbin.org/user-agent",
        "http://httpbin.org/ip",
        "http://ifconfig.me",
    ]

    ALL_DESTINATIONS = DESTINATIONS + ADDITIONAL_TARGETS

    class CrawlerSimulator:
        """Simulates a production crawler with realistic behavior."""
        
        def __init__(self, vendor: str):
            self.vendor = vendor
            self.session = None
            self.stats = {
                'total_requests': 0,
                'successful_requests': 0,
                'failed_requests': 0,
                'total_bytes_sent': 0,
                'total_bytes_received': 0
            }
        
        async def start(self):
            """Initialize the HTTP session with proxy."""
            timeout = aiohttp.ClientTimeout(total=30, connect=10)
            connector = aiohttp.TCPConnector(
                limit=CONCURRENT_REQUESTS,
                limit_per_host=10,
                ttl_dns_cache=300,
                enable_cleanup_closed=True
            )
            
            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={
                    'User-Agent': f'Crawler-{self.vendor}-v1.0',
                    'X-Proxy-Vendor': self.vendor
                }
            )
            logger.info(f"[OK] Crawler initialized for vendor: {self.vendor}")
        
        async def stop(self):
            """Clean shutdown."""
            if self.session:
                await self.session.close()
            logger.info(f"[STOP] Crawler stopped. Stats: {json.dumps(self.stats, indent=2)}")
        
        async def make_request(self, url: str) -> Dict:
            """Make a single HTTP request (Istio intercepts automatically)."""
            start_time = datetime.now()
            
            try:
                # Istio sidecar intercepts automatically - no proxy param needed!
                # Headers are added in session initialization
                async with self.session.get(
                    url,
                    ssl=False,  # For testing; use proper SSL in production
                    allow_redirects=True
                ) as response:
                    content = await response.read()
                    
                    # Update stats
                    self.stats['total_requests'] += 1
                    self.stats['successful_requests'] += 1
                    self.stats['total_bytes_received'] += len(content)
                    
                    duration = (datetime.now() - start_time).total_seconds()
                    
                    logger.info(
                        f"[OK] [{self.vendor}] {response.status} | {url[:50]} | "
                        f"{len(content)} bytes | {duration:.2f}s"
                    )
                    
                    return {
                        'success': True,
                        'status_code': response.status,
                        'bytes_received': len(content),
                        'duration': duration
                    }
            
            except asyncio.TimeoutError:
                self.stats['total_requests'] += 1
                self.stats['failed_requests'] += 1
                logger.warning(f"[TIMEOUT] [{self.vendor}] Timeout: {url}")
                return {'success': False, 'error': 'timeout'}
            
            except Exception as e:
                self.stats['total_requests'] += 1
                self.stats['failed_requests'] += 1
                logger.error(f"[ERROR] [{self.vendor}] Error: {url} - {str(e)}")
                return {'success': False, 'error': str(e)}
        
        async def crawl_continuously(self):
            """Main crawling loop with realistic patterns."""
            logger.info(f"[START] Starting continuous crawling for vendor: {self.vendor}")
            
            while True:
                try:
                    # Select random destination
                    url = random.choice(ALL_DESTINATIONS)
                    
                    # Make request
                    await self.make_request(url)
                    
                    # Realistic delay between requests
                    delay = 1.0 / REQUESTS_PER_SECOND
                    # Add jitter (Â±20%)
                    jitter = delay * random.uniform(-0.2, 0.2)
                    await asyncio.sleep(delay + jitter)
                
                except Exception as e:
                    logger.error(f"[ERROR] Crawl loop error: {e}")
                    await asyncio.sleep(5)  # Back off on errors

    async def report_stats_periodically(crawler: CrawlerSimulator):
        """Report statistics every 60 seconds."""
        while True:
            await asyncio.sleep(60)
            logger.info(
                f"[STATS] [{crawler.vendor}] Stats Report:\n"
                f"  Total Requests: {crawler.stats['total_requests']}\n"
                f"  Successful: {crawler.stats['successful_requests']}\n"
                f"  Failed: {crawler.stats['failed_requests']}\n"
                f"  Success Rate: {crawler.stats['successful_requests'] / max(crawler.stats['total_requests'], 1) * 100:.1f}%\n"
                f"  Bytes Received: {crawler.stats['total_bytes_received']:,}"
            )

    async def main():
        """Main entry point."""
        logger.info(f"[INIT] Load Generator Starting")
        logger.info(f"  Proxy Vendor: {PROXY_VENDOR}")
        logger.info(f"  Envoy Proxy: {ENVOY_PROXY}")
        logger.info(f"  Requests/sec: {REQUESTS_PER_SECOND}")
        logger.info(f"  Concurrent: {CONCURRENT_REQUESTS}")
        
        crawler = CrawlerSimulator(PROXY_VENDOR)
        
        try:
            await crawler.start()
            
            # Start crawling and stats reporting
            await asyncio.gather(
                crawler.crawl_continuously(),
                report_stats_periodically(crawler),
                return_exceptions=True
            )
        
        except KeyboardInterrupt:
            logger.info("[SHUTDOWN] Shutting down gracefully...")
        
        finally:
            await crawler.stop()

    if __name__ == '__main__':
        asyncio.run(main())
---
{{- range .Values.loadGenerator.vendors }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: load-generator-{{ .name }}
  namespace: {{ $.Values.global.crawlersNamespace }}
  labels:
    app: load-generator
    proxy-vendor: {{ .name }}
spec:
  replicas: {{ div $.Values.loadGenerator.replicas (len $.Values.loadGenerator.vendors) | default 1 }}
  selector:
    matchLabels:
      app: load-generator
      proxy-vendor: {{ .name }}
  template:
    metadata:
      labels:
        app: load-generator
        proxy-vendor: {{ .name }}
      annotations:
        prometheus.io/scrape: "false"
    spec:
      # Istio will automatically inject the sidecar proxy
      # No manual Envoy sidecar needed!
      containers:
        # Load generator application
        - name: load-generator
          image: "{{ $.Values.loadGenerator.image.repository }}:{{ $.Values.loadGenerator.image.tag }}"
          imagePullPolicy: {{ $.Values.loadGenerator.image.pullPolicy }}
          command:
            - sh
            - -c
            - |
              pip install --no-cache-dir aiohttp && \
              python /scripts/load-generator.py
          env:
            - name: PROXY_VENDOR
              value: "{{ .name }}"
            - name: REQUESTS_PER_SECOND
              value: "{{ $.Values.loadGenerator.traffic.requestsPerSecond }}"
            - name: CONCURRENT_REQUESTS
              value: "{{ $.Values.loadGenerator.traffic.concurrentRequests }}"
            - name: HTTP_RATIO
              value: "{{ $.Values.loadGenerator.traffic.httpRatio }}"
          resources:
            {{- toYaml $.Values.loadGenerator.resources | nindent 12 }}
          volumeMounts:
            - name: scripts
              mountPath: /scripts
      volumes:
        - name: scripts
          configMap:
            name: load-generator-script
            defaultMode: 0755
---
{{- end }}
{{- if .Values.loadGenerator.autoscaling.enabled }}
{{- range .Values.loadGenerator.vendors }}
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: load-generator-{{ .name }}-hpa
  namespace: {{ $.Values.global.crawlersNamespace }}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: load-generator-{{ .name }}
  minReplicas: {{ $.Values.loadGenerator.autoscaling.minReplicas }}
  maxReplicas: {{ $.Values.loadGenerator.autoscaling.maxReplicas }}
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: {{ $.Values.loadGenerator.autoscaling.targetCPUUtilizationPercentage }}
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: {{ $.Values.loadGenerator.autoscaling.targetMemoryUtilizationPercentage }}
---
{{- end }}
{{- end }}
{{- end }}

